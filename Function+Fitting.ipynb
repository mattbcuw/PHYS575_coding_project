{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Variable Neural Net Simulation\n",
    "\n",
    "This is largely based on https://github.com/XanaduAI/quantum-neural-networks/blob/master/function_fitting/function_fitting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First setting up the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import strawberryfields as sf\n",
    "from strawberryfields.ops import *\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then set some hyperparameters from the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fock basis truncation\n",
    "cutoff = 10\n",
    "# domain [-xmax, xmax] to perform the function fitting over\n",
    "xmax = 1\n",
    "# Number of batches to use in the optimization\n",
    "# Each batch corresponds to a different input-output relation\n",
    "batch_size = 50\n",
    "# Number of photonic quantum layers\n",
    "depth = 6\n",
    "\n",
    "# variable clipping values\n",
    "disp_clip = 1000\n",
    "sq_clip = 50\n",
    "kerr_clip = 50\n",
    "\n",
    "# number of optimization steps\n",
    "reps = 1000\n",
    "\n",
    "# regularization\n",
    "regularization = 0.0\n",
    "reg_variance = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the sine data from the initial experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load('sine_train_data.npy')\n",
    "test_data = np.load('sine_test_data.npy')\n",
    "data_y = np.load('sine_outputs.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the engine up with a single qumode, then define a layer, a net, and start the engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_engine_1():\n",
    "    global cost, loss, mean_x, errors_y, var, state_norm, input_data, output_data\n",
    "    \n",
    "    # Random initialization of gate parameters\n",
    "    sdev = 0.05\n",
    "\n",
    "    with tf.name_scope('variables'):\n",
    "        d_r = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        d_phi = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        r1 = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        sq_r = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        sq_phi = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        r2 = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        kappa1 = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "    \n",
    "    # construct the one-mode Strawberry Fields engine\n",
    "    eng, q = sf.Engine(1)\n",
    "    \n",
    "    def layer(i):\n",
    "        \"\"\"This function generates the ith layer of the quantum neural network.\n",
    "\n",
    "        Note: it must be executed within a Strawberry Fields engine context.\n",
    "\n",
    "        Args:\n",
    "            i (int): the layer number.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('layer_{}'.format(i)):\n",
    "            # displacement gate\n",
    "            Dgate(tf.clip_by_value(d_r[i], -disp_clip, disp_clip), d_phi[i]) | q[0]\n",
    "            # rotation gate\n",
    "            Rgate(r1[i]) | q[0]\n",
    "            # squeeze gate\n",
    "            Sgate(tf.clip_by_value(sq_r[i], -sq_clip, sq_clip), sq_phi[i]) | q[0]\n",
    "            # rotation gate\n",
    "            Rgate(r2[i]) | q[0]\n",
    "            # Kerr gate\n",
    "            Kgate(tf.clip_by_value(kappa1[i], -kerr_clip, kerr_clip)) | q[0]\n",
    "\n",
    "    # Use a TensorFlow placeholder to store the input data\n",
    "    input_data = tf.placeholder(tf.float32, shape=[batch_size])\n",
    "\n",
    "    # construct the circuit\n",
    "    with eng:\n",
    "        # the input data is encoded as displacement in the phase space\n",
    "        Dgate(input_data) | q[0]\n",
    "\n",
    "        for k in range(depth):\n",
    "            # apply layers to the required depth\n",
    "            layer(k)\n",
    "\n",
    "    # run the engine\n",
    "    state = eng.run('tf', cutoff_dim=cutoff, eval=False, batch_size=batch_size)\n",
    "\n",
    "    # First, we calculate the x-quadrature expectation value\n",
    "    ket = state.ket()\n",
    "    mean_x, svd_x = state.quad_expectation(0)\n",
    "    errors_y = tf.sqrt(svd_x)\n",
    "\n",
    "    # the loss function is defined as mean(|<x>[batch_num] - data[batch_num]|^2)\n",
    "    output_data = tf.placeholder(tf.float32, shape=[batch_size])\n",
    "    loss = tf.reduce_mean(tf.abs(mean_x - output_data) ** 2)\n",
    "    var = tf.reduce_mean(errors_y)\n",
    "\n",
    "    # when constructing the cost function, we ensure that the norm of the state\n",
    "    # remains close to 1, and that the variance in the error do not grow.\n",
    "    state_norm = tf.abs(tf.reduce_mean(state.trace()))\n",
    "    cost = loss + regularization * (tf.abs(state_norm - 1) ** 2) + reg_variance*var\n",
    "    tf.summary.scalar('cost', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_engine_2():\n",
    "    global cost, loss, mean_x, errors_y, var, state_norm, input_data, output_data\n",
    "    \n",
    "    # Random initialization of gate parameters\n",
    "    sdev = 0.05\n",
    "\n",
    "    with tf.name_scope('variables'):\n",
    "        d_r = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "        d_phi = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "        r1 = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "        bs1 = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        sq_r = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "        sq_phi = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "        r2 = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "        bs2 = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        kappa1 = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "    \n",
    "    # construct the one-mode Strawberry Fields engine\n",
    "    eng, q = sf.Engine(2)\n",
    "    \n",
    "    def layer(i):\n",
    "        with tf.name_scope('layer_{}'.format(i)):\n",
    "            # displacement gate\n",
    "            Dgate(tf.clip_by_value(d_r[i, 0], -disp_clip, disp_clip), d_phi[i, 0]) | q[0]\n",
    "            Dgate(tf.clip_by_value(d_r[i, 1], -disp_clip, disp_clip), d_phi[i, 1]) | q[1]\n",
    "            # rotation gate\n",
    "            Rgate(r1[i, 0]) | q[0]\n",
    "            Rgate(r1[i, 1]) | q[1]\n",
    "            # beamsplitter\n",
    "            BSgate(bs1[i]) | (q[0], q[1])\n",
    "            # squeeze gate\n",
    "            Sgate(tf.clip_by_value(sq_r[i, 0], -sq_clip, sq_clip), sq_phi[i, 0]) | q[0]\n",
    "            Sgate(tf.clip_by_value(sq_r[i, 1], -sq_clip, sq_clip), sq_phi[i, 1]) | q[1]\n",
    "            # rotation gate\n",
    "            Rgate(r2[i, 0]) | q[0]\n",
    "            Rgate(r2[i, 1]) | q[1]\n",
    "            # beamsplitter\n",
    "            BSgate(bs2[i]) | (q[0], q[1])\n",
    "            # Kerr gate\n",
    "            Kgate(tf.clip_by_value(kappa1[i, 0], -kerr_clip, kerr_clip)) | q[0]\n",
    "            Kgate(tf.clip_by_value(kappa1[i, 1], -kerr_clip, kerr_clip)) | q[1]\n",
    "\n",
    "    # Use a TensorFlow placeholder to store the input data\n",
    "    input_data = tf.placeholder(tf.float32, shape=[batch_size])\n",
    "\n",
    "    # construct the circuit\n",
    "    with eng:\n",
    "        # the input data is encoded as displacement in the phase space\n",
    "        Dgate(input_data) | q[0]\n",
    "        Dgate(input_data) | q[1]\n",
    "\n",
    "        for k in range(depth):\n",
    "            # apply layers to the required depth\n",
    "            layer(k)\n",
    "\n",
    "    # run the engine\n",
    "    state = eng.run('tf', cutoff_dim=cutoff, eval=False, batch_size=batch_size)\n",
    "\n",
    "    # First, we calculate the x-quadrature expectation value\n",
    "    ket = state.ket()\n",
    "    mean_x, svd_x = state.quad_expectation(0)\n",
    "    errors_y = tf.sqrt(svd_x)\n",
    "\n",
    "    # the loss function is defined as mean(|<x>[batch_num] - data[batch_num]|^2)\n",
    "    output_data = tf.placeholder(tf.float32, shape=[batch_size])\n",
    "    loss = tf.reduce_mean(tf.abs(mean_x - output_data) ** 2)\n",
    "    var = tf.reduce_mean(errors_y)\n",
    "\n",
    "    # when constructing the cost function, we ensure that the norm of the state\n",
    "    # remains close to 1, and that the variance in the error do not grow.\n",
    "    state_norm = tf.abs(tf.reduce_mean(state.trace()))\n",
    "    cost = loss + regularization * (tf.abs(state_norm - 1) ** 2) + reg_variance*var\n",
    "    tf.summary.scalar('cost', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f3(x, eps=0.0):\n",
    "    \"\"\"The function f(x)=sin(pi*x)+noise\"\"\"\n",
    "    return 1.0*(np.sin(1.0 * x * np.pi) + eps * np.random.normal(size=x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_state(session, source_function, save_state = False):\n",
    "    predictions = session.run(mean_x, feed_dict={input_data: test_data})\n",
    "\n",
    "    if save_state:\n",
    "        np.save('sine_test_predictions', test_predictions)\n",
    "        print(\"Elapsed time is {} seconds\".format(np.round(end_time - start_time)))\n",
    "\n",
    "    x = np.linspace(-xmax, xmax, 200)\n",
    "\n",
    "    # set plotting options\n",
    "    rcParams['font.family'] = 'serif'\n",
    "    rcParams['font.sans-serif'] = ['Computer Modern Roman']\n",
    "\n",
    "    # Turn interactive plotting off\n",
    "    plt.ioff()\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "\n",
    "    # plot the function to be fitted, in green\n",
    "    ax.plot(x, f3(x), color='#3f9b0b', zorder=1, linewidth=2)\n",
    "\n",
    "    # plot the training data, in red\n",
    "    ax.scatter(train_data, data_y, color='#fb2943', marker='o', zorder=2, s=75)\n",
    "\n",
    "    # plot the test predictions, in blue\n",
    "    ax.scatter(test_data, predictions, color='#0165fc', marker='x', zorder=3, s=75)\n",
    "\n",
    "    ax.set_xlabel('Input', fontsize=18)\n",
    "    ax.set_ylabel('Output', fontsize=18)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training_loop(loss_mod=100, viz_mod=100):\n",
    "    optimiser = tf.train.AdamOptimizer()\n",
    "    min_op = optimiser.minimize(cost)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    loss_vals = []\n",
    "    error_vals = []\n",
    "\n",
    "    # start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(reps+1):\n",
    "\n",
    "        loss_, predictions, errors, mean_error, ket_norm, _ = session.run(\n",
    "            [loss, mean_x, errors_y, var, state_norm, min_op],\n",
    "            feed_dict={input_data: train_data, output_data: data_y})\n",
    "\n",
    "        loss_vals.append(loss_)\n",
    "        error_vals.append(mean_error)\n",
    "\n",
    "        if i % loss_mod == 0:\n",
    "            print('Step: {} Loss: {}'.format(i, loss_))\n",
    "            print(\"Elapsed time is {} seconds\".format(np.round(time.time() - start_time)))\n",
    "        if i % viz_mod == 0:\n",
    "            visualize_state(session, predictions)\n",
    "\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_engine_1()\n",
    "single_time = run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_engine_2()\n",
    "double_time = run_training_loop(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Time for one qumode is {} seconds\".format(np.round(single_time)))\n",
    "print(\"Time for two qumodes is {} seconds\".format(np.round(double_time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
