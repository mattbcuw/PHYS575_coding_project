{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Variable Neural Net Simulation\n",
    "\n",
    "This notebook is based on [this paper](https://arxiv.org/abs/1806.06871), and generalizes some of its results to two dimensions. The [paper's source code](https://github.com/XanaduAI/quantum-neural-networks/blob/master/function_fitting/function_fitting.py) serves as the basis, with modifications to improve presentability and to generalize.\n",
    "\n",
    "Some of the code is organized into large blocks defining important functions so that plots and comparisions of the results can be organized together at the end of the notebook. They do not need to be read in order, and in fact may just as well be read in reverse to provide context, but regardless of their order comments within the code block will be just as helpful as text like this. Comments from the original authors have been stripped out (with the exception of docstrings) for consistency in style and vocabulary, but they can be found in the original source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is setting up the imports. `strawberryfields` is developed by the paper's authors and serves as the simulator of the quantum computer, `tensorflow` provides machine learning capabilities such as gradient descent, and `matplotlib` is used for visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import cm\n",
    "import mpl_toolkits.mplot3d \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import strawberryfields as sf\n",
    "from strawberryfields.ops import *\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwing are hyperparameters for the simulation. They are identical to the source paper with the exception of `reps`, the number of iterations in the training loop, which has been reduced by a factor of 10 for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Maximum dimension of the Fock basis\n",
    "cutoff = 10\n",
    "# Bounding on the visualization\n",
    "xmax = 1\n",
    "# Batch size of the data. In this case the training and test data have the same number of samples\n",
    "batch_size = 50\n",
    "# Number of layers in the neural network.\n",
    "depth = 6\n",
    "\n",
    "# Some bounds for the quantum gates under simulation.\n",
    "disp_clip = 1000\n",
    "sq_clip = 50\n",
    "kerr_clip = 50\n",
    "\n",
    "# Iterations of training\n",
    "reps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating Results in One Dimension\n",
    "\n",
    "These are the data used in the original paper. They are derived from a simple sine function, with the input scaled by pi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load('sine_train_data.npy')\n",
    "test_data = np.load('sine_test_data.npy')\n",
    "data_y = np.load('sine_outputs.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below represents the ideal function that the model hopes to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return np.sin(1.0 * x * np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below sets up the simulation engine with a single qumode. The engine includes the circuit definition that composes the neural network. The theoretical basis and a more complete description of the gates are both described in [the original paper](https://arxiv.org/abs/1806.06871) as well as the accompanying written report, but in general each layer represents both a linear and nonlinear function applied to the input, with the parameters of both configurable by an optimizer. The optimizer experiments with updates that improve the quality of the model, as measured by the loss function. In this case the loss function simply measures the distance between what the model outputs and the true sine function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_engine_1():\n",
    "    global cost, loss, mean_x, error_size, var, state_norm, input_data, output_data\n",
    "    \n",
    "    # Standard deviation of the distribution producing the random initialization\n",
    "    sdev = 0.05\n",
    "\n",
    "    # These are the parameters to be optimized. Each corresponds to one of the gates in the neural net\n",
    "    # They are organized by time. E.g., all displacement gates are configured by d_r and d_phi\n",
    "    with tf.name_scope('variables'):\n",
    "        d_r = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        d_phi = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        r1 = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        sq_r = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        sq_phi = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        r2 = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        kappa1 = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "    \n",
    "    # Initialize Strawberry Fields with a single qumode\n",
    "    eng, q = sf.Engine(1)\n",
    "    \n",
    "    def layer(i):\n",
    "        \"\"\"This function generates the ith layer of the quantum neural network.\n",
    "\n",
    "        Note: it must be executed within a Strawberry Fields engine context.\n",
    "\n",
    "        Args:\n",
    "            i (int): the layer number.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('layer_{}'.format(i)):\n",
    "            # Displacement\n",
    "            Dgate(tf.clip_by_value(d_r[i], -disp_clip, disp_clip), d_phi[i]) | q[0]\n",
    "            # Rotation\n",
    "            # his is technically a component of an interfermoter including a beamsplitter, but that is a 2-qumode gate\n",
    "            Rgate(r1[i]) | q[0]\n",
    "            # Squeezing\n",
    "            Sgate(tf.clip_by_value(sq_r[i], -sq_clip, sq_clip), sq_phi[i]) | q[0]\n",
    "            # Second rotatin with the same caveat as above\n",
    "            Rgate(r2[i]) | q[0]\n",
    "            # The \"nonlinearity\", in this case a Kerr Gate\n",
    "            Kgate(tf.clip_by_value(kappa1[i], -kerr_clip, kerr_clip)) | q[0]\n",
    "\n",
    "    # This variable holds the inputs to the model\n",
    "    input_data = tf.placeholder(tf.float32, shape=[batch_size])\n",
    "\n",
    "    # This is the circuit definition. The code inside the block is technically in the Blackbird language\n",
    "    # Blackbird is a Strawberry Fields construct, and more information can be found in the paper describing it\n",
    "    # https://arxiv.org/abs/1804.03159\n",
    "    with eng:\n",
    "        # The input data are encoded as a simple displacement of the qumode.\n",
    "        Dgate(input_data) | q[0]\n",
    "\n",
    "        # The neural net is formed by applying the layers in sequence\n",
    "        for k in range(depth):\n",
    "            layer(k)\n",
    "\n",
    "    # This runs the simulation defined above\n",
    "    state = eng.run('tf', cutoff_dim=cutoff, eval=False, batch_size=batch_size)\n",
    "\n",
    "    # This code measures the output, specifically quad_expectation\n",
    "    # In this case the mean is taken to be the prediction and the standard deviation the confidence\n",
    "    ket = state.ket()\n",
    "    mean_x, svd_x = state.quad_expectation(0)\n",
    "    error_size = tf.sqrt(svd_x)\n",
    "\n",
    "    # The loss function is taken to be a simple mean squared error\n",
    "    output_data = tf.placeholder(tf.float32, shape=[batch_size])\n",
    "    loss = tf.reduce_mean(tf.abs(mean_x - output_data) ** 2)\n",
    "    var = tf.reduce_mean(error_size)\n",
    "\n",
    "    # This code makes the loss available to the optimizer.\n",
    "    state_norm = tf.abs(tf.reduce_mean(state.trace()))\n",
    "    tf.summary.scalar('cost', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the simulation set up, it is now possible to train and assess the model. The code below is a utility function to visualize the data to make the model's progress more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_state(session, save_state = False):\n",
    "    \n",
    "    # Take predictions from the current state of the model\n",
    "    predictions = session.run(mean_x, feed_dict={input_data: test_data})\n",
    "\n",
    "    if save_state:\n",
    "        np.save('sine_test_predictions', test_predictions)\n",
    "        print(\"Elapsed time is {} seconds\".format(np.round(end_time - start_time)))\n",
    "\n",
    "    # Get an arbitrary set of inputs to make a nice curve\n",
    "    x = np.linspace(-xmax, xmax, 200)\n",
    "\n",
    "    # Set up fonts\n",
    "    rcParams['font.family'] = 'serif'\n",
    "    rcParams['font.sans-serif'] = ['Computer Modern Roman']\n",
    "\n",
    "    # Turn interactive plotting off to avoid unexpected empty plots\n",
    "    plt.ioff()\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "\n",
    "    # Plot the ideal\n",
    "    ax.plot(x, f1(x), color='#3f9b0b', zorder=1, linewidth=2)\n",
    "\n",
    "    # Plot the training data, which deviate from the ideal randomly (and intentionaly)\n",
    "    ax.scatter(train_data, data_y, color='#fb2943', marker='o', zorder=2, s=75)\n",
    "\n",
    "    # Plot the model's current predictions\n",
    "    ax.scatter(test_data, predictions, color='#0165fc', marker='x', zorder=3, s=75)\n",
    "\n",
    "    ax.set_xlabel('Input', fontsize=18)\n",
    "    ax.set_ylabel('Output', fontsize=18)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizing to Two Dimensions\n",
    "\n",
    "The code below generalizes the above result to two dimensions, which demonstrates both the increase in computational complexity, and the complete description of the neural net from the original paper, including beamsplitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load('sine_2d_train_input.npy')\n",
    "test_data = np.load('sine_2d_test_input.npy')\n",
    "data_y = np.load('sine_2d_train_output.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f2(x, y):\n",
    "    return np.sin(x * np.pi) + np.cos(y * np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_engine_2():\n",
    "    global cost, loss, mean_x, error_size, var, state_norm, input_data, output_data\n",
    "    \n",
    "    # Random initialization of gate parameters\n",
    "    sdev = 0.05\n",
    "\n",
    "    with tf.name_scope('variables'):\n",
    "        d_r = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "        d_phi = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "        r1 = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "        bs1 = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        sq_r = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "        sq_phi = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "        r2 = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "        bs2 = tf.Variable(tf.random_normal(shape=[depth], stddev=sdev))\n",
    "        kappa1 = tf.Variable(tf.random_normal(shape=[depth, 2], stddev=sdev))\n",
    "    \n",
    "    # construct the one-mode Strawberry Fields engine\n",
    "    eng, q = sf.Engine(2)\n",
    "    \n",
    "    def layer(i):\n",
    "        with tf.name_scope('layer_{}'.format(i)):\n",
    "            # displacement gate\n",
    "            Dgate(tf.clip_by_value(d_r[i, 0], -disp_clip, disp_clip), d_phi[i, 0]) | q[0]\n",
    "            Dgate(tf.clip_by_value(d_r[i, 1], -disp_clip, disp_clip), d_phi[i, 1]) | q[1]\n",
    "            # rotation gate\n",
    "            Rgate(r1[i, 0]) | q[0]\n",
    "            Rgate(r1[i, 1]) | q[1]\n",
    "            # beamsplitter\n",
    "            BSgate(bs1[i]) | (q[0], q[1])\n",
    "            # squeeze gate\n",
    "            Sgate(tf.clip_by_value(sq_r[i, 0], -sq_clip, sq_clip), sq_phi[i, 0]) | q[0]\n",
    "            Sgate(tf.clip_by_value(sq_r[i, 1], -sq_clip, sq_clip), sq_phi[i, 1]) | q[1]\n",
    "            # rotation gate\n",
    "            Rgate(r2[i, 0]) | q[0]\n",
    "            Rgate(r2[i, 1]) | q[1]\n",
    "            # beamsplitter\n",
    "            BSgate(bs2[i]) | (q[0], q[1])\n",
    "            # Kerr gate\n",
    "            Kgate(tf.clip_by_value(kappa1[i, 0], -kerr_clip, kerr_clip)) | q[0]\n",
    "            Kgate(tf.clip_by_value(kappa1[i, 1], -kerr_clip, kerr_clip)) | q[1]\n",
    "\n",
    "    # Use a TensorFlow placeholder to store the input data\n",
    "    input_data = tf.placeholder(tf.float32, shape=[batch_size, 2])\n",
    "\n",
    "    # construct the circuit\n",
    "    with eng:\n",
    "        # the input data is encoded as displacement in the phase space\n",
    "        Dgate(input_data[]) | q[0]\n",
    "        Dgate(input_data) | q[1]\n",
    "\n",
    "        for k in range(depth):\n",
    "            # apply layers to the required depth\n",
    "            layer(k)\n",
    "\n",
    "    # run the engine\n",
    "    state = eng.run('tf', cutoff_dim=cutoff, eval=False, batch_size=batch_size)\n",
    "\n",
    "    # First, we calculate the x-quadrature expectation value\n",
    "    ket = state.ket()\n",
    "    # Noting input as z and y and output as x is inconsistent with the plots, but simplifies the code\n",
    "    mean_z, svd_z = state.quad_expectation(0)\n",
    "    mean_y, svd_y = state.quad_expectation(1)\n",
    "    # By simply summing the two we allow one dimension to learn sine and the other cosine\n",
    "    mean_x = mean_z + mean_y\n",
    "    error_size = tf.sqrt(svd_z) + tf.sqrt(svd_y)\n",
    "\n",
    "    # the loss function is defined as mean(|<x>[batch_num] - data[batch_num]|^2)\n",
    "    output_data = tf.placeholder(tf.float32, shape=[batch_size])\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.abs(mean_z - output_data) ** 2)\n",
    "    var = tf.reduce_mean(error_size)\n",
    "\n",
    "    # when constructing the cost function, we ensure that the norm of the state\n",
    "    # remains close to 1, and that the variance in the error do not grow.\n",
    "    state_norm = tf.abs(tf.reduce_mean(state.trace()))\n",
    "    tf.summary.scalar('cost', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_state_3d(session, save_state = False):\n",
    "    \n",
    "    # As above, keep the x notation to be consistent in code, but plot this on the z-axis\n",
    "    predictions = session.run(mean_x, feed_dict={input_data: test_data})\n",
    "\n",
    "    if save_state:\n",
    "        np.save('sine_test_predictions', test_predictions)\n",
    "        print(\"Elapsed time is {} seconds\".format(np.round(end_time - start_time)))\n",
    "\n",
    "    # set plotting options\n",
    "    rcParams['font.family'] = 'serif'\n",
    "    rcParams['font.sans-serif'] = ['Computer Modern Roman']\n",
    "\n",
    "    plt.ioff()\n",
    "    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "    # Borrowing from https://matplotlib.org/stable/gallery/mplot3d/surface3d.html\n",
    "    X = np.arange(-1, 1, 0.1)\n",
    "    Y = np.arange(-1, 1, 0.1)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    Z = f2(X, Y)\n",
    "\n",
    "    # Plot the surface.\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=cm.Greys,\n",
    "                           linewidth=0, antialiased=False)\n",
    "\n",
    "    # plot the training data, in red\n",
    "    ax.scatter3D(train_data[:,0],train_data[:,1], data_y, color='#fb2943', marker='o', zorder=2, s=75)\n",
    "\n",
    "    # plot the test predictions, in blue\n",
    "    ax.scatter(test_data[:, 0], test_data[:, 1], predictions, color='#0165fc', marker='x', zorder=3, s=75)\n",
    "    ax.set_xlabel('Input', fontsize=18)\n",
    "    ax.set_ylabel('Output', fontsize=18)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default the learning rate to the TF default: https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdamOptimizer\n",
    "def run_training_loop(loss_mod=100, viz_mod=100, use3d=False, learning_rate=0.001):\n",
    "    global session\n",
    "    \n",
    "    optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    min_op = optimiser.minimize(cost)\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    loss_vals = []\n",
    "    error_vals = []\n",
    "\n",
    "    # start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(reps+1):\n",
    "\n",
    "        loss_, predictions, errors, mean_error, ket_norm, _ = session.run(\n",
    "            [loss, mean_x, error_size, var, state_norm, min_op],\n",
    "            feed_dict={input_data: train_data, output_data: data_y})\n",
    "\n",
    "        loss_vals.append(loss_)\n",
    "        error_vals.append(mean_error)\n",
    "\n",
    "        if i % loss_mod == 0:\n",
    "            print('Step: {} Loss: {}'.format(i, loss_))\n",
    "            print(\"Elapsed time is {} seconds\".format(np.round(time.time() - start_time)))\n",
    "        if i % viz_mod == 0:\n",
    "            if use3d:\n",
    "                visualize_state_3d(session)\n",
    "            else:\n",
    "                visualize_state(session)\n",
    "\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_engine_1()\n",
    "single_time = run_training_loop(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_engine_2()\n",
    "double_time = run_training_loop(1, 10, True, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Time for one qumode is {} seconds\".format(np.round(single_time)))\n",
    "print(\"Time for two qumodes is {} seconds\".format(np.round(double_time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
